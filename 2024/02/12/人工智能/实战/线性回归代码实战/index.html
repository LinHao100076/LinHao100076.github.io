<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>线性回归代码实战 | LinHao's Pages</title><meta name="author" content="LinHao"><meta name="copyright" content="LinHao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="线性回归代码实战引入库函数1234### 引入库函数  import torch  import matplotlib.pyplot as plt     # 作图  import random   # 产生随机数 在这一步中，我们引入了三个库：torch 是 PyTorch 深度学习框架，用于构建和训练模型；matplotlib.pyplot 用于绘制图形，方便我们可视化数据和模型结果；rand">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归代码实战">
<meta property="og:url" content="http://example.com/2024/02/12/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%AE%9E%E6%88%98/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/index.html">
<meta property="og:site_name" content="LinHao&#39;s Pages">
<meta property="og:description" content="线性回归代码实战引入库函数1234### 引入库函数  import torch  import matplotlib.pyplot as plt     # 作图  import random   # 产生随机数 在这一步中，我们引入了三个库：torch 是 PyTorch 深度学习框架，用于构建和训练模型；matplotlib.pyplot 用于绘制图形，方便我们可视化数据和模型结果；rand">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/cover/1.jpg">
<meta property="article:published_time" content="2024-02-11T16:00:00.000Z">
<meta property="article:modified_time" content="2025-05-14T01:48:52.097Z">
<meta property="article:author" content="LinHao">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/cover/1.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "线性回归代码实战",
  "url": "http://example.com/2024/02/12/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%AE%9E%E6%88%98/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/",
  "image": "http://example.com/img/cover/1.jpg",
  "datePublished": "2024-02-11T16:00:00.000Z",
  "dateModified": "2025-05-14T01:48:52.097Z",
  "author": [
    {
      "@type": "Person",
      "name": "LinHao",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/02/12/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%AE%9E%E6%88%98/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '线性回归代码实战',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/wallpaper/1.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar/3.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/share/"><i class="fa-fw fa fa-address-book"></i><span> 分享</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-solid fa-image"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fa fa-gamepad"></i><span> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">LinHao's Pages</span></a><a class="nav-page-title" href="/"><span class="site-name">线性回归代码实战</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/share/"><i class="fa-fw fa fa-address-book"></i><span> 分享</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fa fa-solid fa-image"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li><li><a class="site-page child" href="/game/"><i class="fa-fw fa fa-gamepad"></i><span> 游戏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">线性回归代码实战</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-11T16:00:00.000Z" title="发表于 2024-02-12 00:00:00">2024-02-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-14T01:48:52.097Z" title="更新于 2025-05-14 09:48:52">2025-05-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/">深度学习项目</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h1 id="线性回归代码实战"><a href="#线性回归代码实战" class="headerlink" title="线性回归代码实战"></a>线性回归代码实战</h1><h2 id="引入库函数"><a href="#引入库函数" class="headerlink" title="引入库函数"></a>引入库函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 引入库函数  </span></span><br><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt     <span class="comment"># 作图  </span></span><br><span class="line"><span class="keyword">import</span> random   <span class="comment"># 产生随机数</span></span><br></pre></td></tr></table></figure>
<p>在这一步中，我们引入了三个库：<code>torch</code> 是 PyTorch 深度学习框架，用于构建和训练模型；<code>matplotlib.pyplot</code> 用于绘制图形，方便我们可视化数据和模型结果；<code>random</code> 用于生成随机数，在后续的数据处理中会用到。</p>
<h2 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 生成数据集 输入真实 w 和 b以及数据量data_num</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generateDataSet</span>(<span class="params">w, b, data_num</span>):  </span><br><span class="line">    x = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (data_num, <span class="built_in">len</span>(w)))  <span class="comment"># std: 0, mean: 1, shape()</span></span><br><span class="line">    y = torch.matmul(x, w) + b          <span class="comment"># matmul函数实现矩阵相乘  </span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 添加噪声  </span></span><br><span class="line">    noise = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)  </span><br><span class="line">    y += noise  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> x, y  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义真实 w 和 b</span></span><br><span class="line">w_true = torch.tensor([<span class="number">8.1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>])  </span><br><span class="line">b_true = torch.tensor(<span class="number">1.1</span>)  </span><br><span class="line">DataSetNum = <span class="number">500</span>  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成训练集</span></span><br><span class="line">X, Y = generateDataSet(w_true, b_true, DataSetNum)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化分享</span></span><br><span class="line">plt.scatter(X[:, <span class="number">3</span>], Y, <span class="number">1</span>)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>这里我们定义了一个函数 <code>generateDataSet</code> 用于生成数据集。首先，我们使用 <code>torch.normal</code> 函数生成均值为 0，标准差为 1 的正态分布随机数作为输入特征 <code>x</code>，其形状为 <code>(data_num, len(w))</code>。然后，通过矩阵乘法 <code>torch.matmul</code> 计算 <code>y</code> 的值，即 <code>y = x * w + b</code>。接着，为了模拟真实数据中的噪声，我们给 <code>y</code> 加上一个均值为 0，标准差为 0.01 的正态分布噪声。最后，返回生成的 <code>x</code> 和 <code>y</code>。</p>
<h2 id="分小批次采集数据"><a href="#分小批次采集数据" class="headerlink" title="分小批次采集数据"></a>分小批次采集数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取数函数, Data: 自变量， labal: 标签, batchsize: 批量大小</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getData</span>(<span class="params">data, label, batch_size</span>):</span><br><span class="line">    length = <span class="built_in">len</span>(label)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(length))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打乱数据, 取数据时要打乱数据</span></span><br><span class="line">    random.shuffle(indices)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, length, batch_size):</span><br><span class="line">        get_indices = indices[each: each + batch_size]</span><br><span class="line">        get_data = data[get_indices]</span><br><span class="line">        get_labal = label[get_indices]</span><br><span class="line">        <span class="comment"># print(f&quot;batch form &#123;get_indices[0]&#125; to &#123;get_indices[-1]&#125;&quot;)</span></span><br><span class="line">        <span class="comment"># return 直接终止函数， yield 有存档点的返回数据</span></span><br><span class="line">        <span class="keyword">yield</span> get_data, get_labal</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试获取数据是否分批</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"><span class="comment"># for batch_x, batch_y in getData(X, Y, batch_size):</span></span><br><span class="line"><span class="comment">#     print(batch_x, batch_y)</span></span><br></pre></td></tr></table></figure>
<p><code>getData</code> 函数用于分小批次采集数据。首先，我们获取标签的长度，生成一个包含所有索引的列表 <code>indices</code>，并使用 <code>random.shuffle</code> 函数打乱这些索引。然后，通过循环每次取出 <code>batch_size</code> 个索引，根据这些索引从数据和标签中取出对应的小批次数据，使用 <code>yield</code> 关键字返回，这样可以实现数据的分批迭代</p>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">x, w, b</span>):</span><br><span class="line">    y_pred = torch.matmul(x, w) + b</span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<p>模型定义非常简单，就是一个线性模型 <code>y_pred = x * w + b</code>，其中 <code>x</code> 是输入特征，<code>w</code> 是权重，<code>b</code> 是偏置。</p>
<h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">maeloss</span>(<span class="params">y_pred, y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="built_in">abs</span>(y_pred - y)) / <span class="built_in">len</span>(y)</span><br></pre></td></tr></table></figure>
<p>这里我们使用平均绝对误差（MAE）作为损失函数，计算预测值 <code>y_pred</code> 与真实值 <code>y</code> 之间的绝对误差的平均值。</p>
<h2 id="梯度下降-SGD-更新参数"><a href="#梯度下降-SGD-更新参数" class="headerlink" title="梯度下降(SGD)更新参数"></a>梯度下降(SGD)更新参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">params, lr</span>):</span><br><span class="line">    <span class="comment"># 不计算梯度</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            <span class="comment"># 无法写成 param = param - param.grad() * lr 报错</span></span><br><span class="line">            param -= param.grad * lr</span><br><span class="line">            <span class="comment"># 将使用过的参数梯度归零</span></span><br><span class="line">            param.grad.zero_()</span><br></pre></td></tr></table></figure>
<p><code>SGD</code> 函数实现了随机梯度下降算法。在更新参数时，我们使用 <code>with torch.no_grad()</code> 上下文管理器来避免计算梯度，因为在更新参数时不需要计算梯度。对于每个参数，我们使用 <code>param -= param.grad * lr</code> 来更新参数值，其中 <code>lr</code> 是学习率。更新完参数后，使用 <code>param.grad.zero_()</code> 将参数的梯度归零，以便下一次计算梯度。</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line"><span class="comment"># 随机生成 w, b参数的值</span></span><br><span class="line"><span class="comment"># 确定累计梯度</span></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, w_true.shape, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">0.01</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># print(w, b)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练轮数</span></span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    data_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_batch, y_batch <span class="keyword">in</span> getData(X, Y, batch_size):</span><br><span class="line">        y_pred = model(x_batch, w, b)</span><br><span class="line">        loss = maeloss(y_pred, y_batch)</span><br><span class="line">        <span class="comment"># 梯度回传</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 更新模型</span></span><br><span class="line">        SGD([w, b], lr)</span><br><span class="line">        data_loss += loss</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epoch: %03d, loss: %.6f&quot;</span>%(epoch, data_loss))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;真实的参数为：&quot;</span>, w_true, b_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练之后的参数：&quot;</span>, w, b)</span><br></pre></td></tr></table></figure>
<p>在训练模型部分，我们首先定义了学习率 <code>lr</code>，然后随机初始化权重 <code>w</code> 和偏置 <code>b</code>，并设置 <code>requires_grad=True</code> 以便计算梯度。接着，我们定义了训练轮数 <code>epochs</code>，在每个训练轮次中，我们遍历每个小批次的数据，计算预测值 <code>y_pred</code> 和损失 <code>loss</code>，使用 <code>loss.backward()</code> 进行梯度回传，然后调用 <code>SGD</code> 函数更新模型参数。最后，打印每个轮次的损失值以及真实参数和训练后的参数。</p>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">idx = <span class="number">1</span></span><br><span class="line">plt.plot(X[:, idx].detach().numpy(), X[:, idx].detach().numpy() * w[idx].detach().numpy() + b.detach().numpy())</span><br><span class="line">plt.scatter(X[:, idx], Y, <span class="number">1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>在可视化部分，我们选择第 <code>idx</code> 个特征，绘制出训练后的模型预测的直线和原始数据的散点图，方便我们直观地观察模型的拟合效果。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>小批次采样时要打乱数据，保证是没有顺序依赖，打乱数据<code>random.shuffle(list)</code></li>
<li><code>with torch.no_grad():</code>作用域下在计算图中不会累计梯度</li>
<li>计算完梯度使用<code>param.grad.zero_()</code>清空梯度否则后续计算梯度会累积梯度(梯度相加)</li>
<li>要计算梯度的参数在定义时，使用<code>requires_grad = True</code>来存储梯度</li>
<li>前向传播计算损失函数，后向传播更新参数</li>
<li>学习率<code>lr</code>较小时训练的模型loss下降缓慢，较大时loss不稳定</li>
<li><code>loss.backward()</code>梯度回传，此时param.grad为$\frac{\partial loss}{\partial param}$</li>
</ul>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 引入库函数  </span></span><br><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt     <span class="comment"># 作图  </span></span><br><span class="line"><span class="keyword">import</span> random   <span class="comment"># 产生随机数  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment">### 生成数据集 输入真实 w 和 b以及数据量  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generateDataSet</span>(<span class="params">w, b, data_num</span>):  </span><br><span class="line">    x = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (data_num, <span class="built_in">len</span>(w)))  </span><br><span class="line">    y = torch.matmul(x, w) + b          <span class="comment"># matmul函数实现矩阵相乘  </span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 添加噪声  </span></span><br><span class="line">    noise = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)  </span><br><span class="line">    y += noise  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> x, y  </span><br><span class="line">  </span><br><span class="line">w_true = torch.tensor([<span class="number">8.1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>])  </span><br><span class="line">b_true = torch.tensor(<span class="number">1.1</span>)  </span><br><span class="line">DataSetNum = <span class="number">500</span>  </span><br><span class="line">  </span><br><span class="line">X, Y = generateDataSet(w_true, b_true, DataSetNum)  </span><br><span class="line">  </span><br><span class="line">plt.scatter(X[:, <span class="number">3</span>], Y, <span class="number">1</span>)  </span><br><span class="line">plt.show()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 取数函数, Data: 自变量， labal: 标签, batchsize: 批量大小  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getData</span>(<span class="params">data, label, batch_size</span>):  </span><br><span class="line">    length = <span class="built_in">len</span>(label)  </span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(length))  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 打乱数据, 取数据时要打乱数据  </span></span><br><span class="line">    random.shuffle(indices)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, length, batch_size):  </span><br><span class="line">        get_indices = indices[each: each + batch_size]  </span><br><span class="line">        get_data = data[get_indices]  </span><br><span class="line">        get_labal = label[get_indices]  </span><br><span class="line">        <span class="comment"># print(f&quot;batch form &#123;get_indices[0]&#125; to &#123;get_indices[-1]&#125;&quot;)  </span></span><br><span class="line">        <span class="comment"># return 直接终止函数， yield 有存档点的返回数据  </span></span><br><span class="line">        <span class="keyword">yield</span> get_data, get_labal  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 测试获取数据是否分批  </span></span><br><span class="line">batch_size = <span class="number">16</span>  </span><br><span class="line"><span class="comment"># for batch_x, batch_y in getData(X, Y, batch_size):  </span></span><br><span class="line"><span class="comment">#     print(batch_x, batch_y)  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># 定义模型  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">x, w, b</span>):  </span><br><span class="line">    y_pred = torch.matmul(x, w) + b  </span><br><span class="line">    <span class="keyword">return</span> y_pred  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 定义损失函数  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">maeloss</span>(<span class="params">y_pred, y</span>):  </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="built_in">abs</span>(y_pred - y)) / <span class="built_in">len</span>(y)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 梯度下降  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">params, lr</span>):  </span><br><span class="line">    <span class="comment"># 不计算梯度  </span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  </span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:  </span><br><span class="line">            <span class="comment"># 无法写成 param = param - param.grad() * lr 报错  </span></span><br><span class="line">            param -= param.grad * lr  </span><br><span class="line">            <span class="comment"># 将使用过的参数梯度归零  </span></span><br><span class="line">            param.grad.zero_()  </span><br><span class="line">  </span><br><span class="line">lr = <span class="number">0.03</span>  </span><br><span class="line"><span class="comment"># 随机生成 w, b参数的值  </span></span><br><span class="line"><span class="comment"># 确定累计梯度  </span></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, w_true.shape, requires_grad=<span class="literal">True</span>)  </span><br><span class="line">b = torch.tensor(<span class="number">0.01</span>, requires_grad=<span class="literal">True</span>)  </span><br><span class="line"><span class="comment"># print(w, b)  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># 训练轮数  </span></span><br><span class="line">epochs = <span class="number">50</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):  </span><br><span class="line">    data_loss = <span class="number">0</span>  </span><br><span class="line">    <span class="keyword">for</span> x_batch, y_batch <span class="keyword">in</span> getData(X, Y, batch_size):  </span><br><span class="line">        y_pred = model(x_batch, w, b)  </span><br><span class="line">        loss = maeloss(y_pred, y_batch)  </span><br><span class="line">        <span class="comment"># 梯度回传  </span></span><br><span class="line">        loss.backward()  </span><br><span class="line">        <span class="comment"># 更新模型  </span></span><br><span class="line">        SGD([w, b], lr)  </span><br><span class="line">        data_loss += loss  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epoch: %03d, loss: %.6f&quot;</span>%(epoch, data_loss))  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;真实的参数为：&quot;</span>, w_true, b_true)  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练之后的参数：&quot;</span>, w, b)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 可视化  </span></span><br><span class="line">idx = <span class="number">1</span>  </span><br><span class="line">plt.plot(X[:, idx].detach().numpy(), X[:, idx].detach().numpy() * w[idx].detach().numpy() + b.detach().numpy())  </span><br><span class="line">plt.scatter(X[:, idx], Y, <span class="number">1</span>)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">LinHao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/02/12/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%AE%9E%E6%88%98/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/">http://example.com/2024/02/12/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%AE%9E%E6%88%98/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">LinHao's Pages</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2023/07/14/hello-world/" title="Hello World"><img class="cover" src="/img/cover/3.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Hello World</div></div><div class="info-2"><div class="info-item-1">Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot;  More info: Writing Run server1$ hexo server  More info: Server Generate static files1$ hexo generate  More info: Generating Deploy to remote sites1$ hexo deploy  More info: Deployment </div></div></div></a><a class="pagination-related" href="/2024/02/13/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%AE%9E%E6%88%98/%E5%9B%9E%E5%BD%92%E9%A2%84%E6%B5%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/" title="回归预测代码实战"><img class="cover" src="/img/cover/2.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">回归预测代码实战</div></div><div class="info-2"><div class="info-item-1">回归预测代码实战引言在机器学习领域，回归分析是一种非常重要的方法，常用于预测连续型变量。本次实战项目以新冠模型预测(ML2021Spring-hw1)为例，展示如何进行数据处理、模型定义、训练以及预测。 项目环境与依赖库123456789101112import timeimport torchimport matplotlib.pyplot as pltimport csvimport numpy as npimport pandas as pdfrom torch.utils.data import DataLoader, Datasetimport torch.nn as nn  # 引入NN模型import torch.optim as optimfrom sklearn.feature_selection import SelectKBest, chi2  torch：深度学习框架，用于构建和训练神经网络模型。 matplotlib.pyplot：用于数据可视化。 csv：用于处理 CSV...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/02/13/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%AE%9E%E6%88%98/%E5%9B%9E%E5%BD%92%E9%A2%84%E6%B5%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/" title="回归预测代码实战"><img class="cover" src="/img/cover/2.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-13</div><div class="info-item-2">回归预测代码实战</div></div><div class="info-2"><div class="info-item-1">回归预测代码实战引言在机器学习领域，回归分析是一种非常重要的方法，常用于预测连续型变量。本次实战项目以新冠模型预测(ML2021Spring-hw1)为例，展示如何进行数据处理、模型定义、训练以及预测。 项目环境与依赖库123456789101112import timeimport torchimport matplotlib.pyplot as pltimport csvimport numpy as npimport pandas as pdfrom torch.utils.data import DataLoader, Datasetimport torch.nn as nn  # 引入NN模型import torch.optim as optimfrom sklearn.feature_selection import SelectKBest, chi2  torch：深度学习框架，用于构建和训练神经网络模型。 matplotlib.pyplot：用于数据可视化。 csv：用于处理 CSV...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar/3.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">LinHao</div><div class="author-info-description">ヾ (・ω・`) o muuua~</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/LinHao100076"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/LinHao100076" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:LinHao100076@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">System.out.print("有些梦虽然遥不可及，但并不是不可能实现!")</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98"><span class="toc-number">1.</span> <span class="toc-text">线性回归代码实战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E5%BA%93%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.</span> <span class="toc-text">引入库函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.2.</span> <span class="toc-text">生成数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B0%8F%E6%89%B9%E6%AC%A1%E9%87%87%E9%9B%86%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.</span> <span class="toc-text">分小批次采集数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.</span> <span class="toc-text">定义损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-SGD-%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0"><span class="toc-number">1.6.</span> <span class="toc-text">梯度下降(SGD)更新参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.7.</span> <span class="toc-text">训练模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.8.</span> <span class="toc-text">可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.9.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-number">1.10.</span> <span class="toc-text">完整代码</span></a></li></ol></li></ol></div></div><div class="card-widget card-post-series"><div class="item-headline"><i class="fa-solid fa-layer-group"></i><span>系列文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/02/13/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%AE%9E%E6%88%98/%E5%9B%9E%E5%BD%92%E9%A2%84%E6%B5%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/" title="回归预测代码实战"><img src="/img/cover/2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="回归预测代码实战"></a><div class="content"><a class="title" href="/2024/02/13/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%AE%9E%E6%88%98/%E5%9B%9E%E5%BD%92%E9%A2%84%E6%B5%8B%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/" title="回归预测代码实战">回归预测代码实战</a><time datetime="2024-02-12T16:00:00.000Z" title="发表于 2024-02-13 00:00:00">2024-02-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/12/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%AE%9E%E6%88%98/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/" title="线性回归代码实战"><img src="/img/cover/1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="线性回归代码实战"></a><div class="content"><a class="title" href="/2024/02/12/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E5%AE%9E%E6%88%98/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/" title="线性回归代码实战">线性回归代码实战</a><time datetime="2024-02-11T16:00:00.000Z" title="发表于 2024-02-12 00:00:00">2024-02-12</time></div></div></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/05/31/Linux%20%E8%BF%90%E7%BB%B4/%E7%BA%A2%E5%B8%BDRHCE/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D/" title="计算机功能介绍"><img src="/img/cover/3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机功能介绍"/></a><div class="content"><a class="title" href="/2025/05/31/Linux%20%E8%BF%90%E7%BB%B4/%E7%BA%A2%E5%B8%BDRHCE/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D/" title="计算机功能介绍">计算机功能介绍</a><time datetime="2025-05-31T13:22:24.858Z" title="发表于 2025-05-31 21:22:24">2025-05-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/23/%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/ADB%E6%97%A0%E7%BA%BF%E8%B0%83%E8%AF%95%E6%89%8B%E6%9C%BA/" title="ADB无线调试手机"><img src="/img/cover/2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ADB无线调试手机"/></a><div class="content"><a class="title" href="/2025/05/23/%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/ADB%E6%97%A0%E7%BA%BF%E8%B0%83%E8%AF%95%E6%89%8B%E6%9C%BA/" title="ADB无线调试手机">ADB无线调试手机</a><time datetime="2025-05-23T07:24:09.045Z" title="发表于 2025-05-23 15:24:09">2025-05-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/15/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%A0%94%E7%A9%B6/UNet%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%AE%B6%E6%97%8F/" title="UNet网络及其家族"><img src="/img/cover/1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="UNet网络及其家族"/></a><div class="content"><a class="title" href="/2025/05/15/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%A0%94%E7%A9%B6/UNet%E7%BD%91%E7%BB%9C%E5%8F%8A%E5%85%B6%E5%AE%B6%E6%97%8F/" title="UNet网络及其家族">UNet网络及其家族</a><time datetime="2025-05-15T12:02:25.254Z" title="发表于 2025-05-15 20:02:25">2025-05-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/30/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%AE%97%E6%B3%95/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" title="线性回归"><img src="/img/cover/3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="线性回归"/></a><div class="content"><a class="title" href="/2025/04/30/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%AE%97%E6%B3%95/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" title="线性回归">线性回归</a><time datetime="2025-04-30T11:18:09.053Z" title="发表于 2025-04-30 19:18:09">2025-04-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/19/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%AE%97%E6%B3%95/KNN%E7%AE%97%E6%B3%95/" title="KNN算法"><img src="/img/cover/2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="KNN算法"/></a><div class="content"><a class="title" href="/2025/04/19/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E7%AE%97%E6%B3%95/KNN%E7%AE%97%E6%B3%95/" title="KNN算法">KNN算法</a><time datetime="2025-04-19T10:34:55.379Z" title="发表于 2025-04-19 18:34:55">2025-04-19</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By LinHao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title></title>
    <link href="/2025/02/11/LinHaoPages/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/"/>
    <url>/2025/02/11/LinHaoPages/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98/</url>
    
    <content type="html"><![CDATA[<h1 id="线性回归代码实战"><a href="#线性回归代码实战" class="headerlink" title="线性回归代码实战"></a>线性回归代码实战</h1><h3 id="Step-1-引入库函数"><a href="#Step-1-引入库函数" class="headerlink" title="Step 1: 引入库函数"></a>Step 1: 引入库函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">### 引入库函数  </span><br><span class="hljs-keyword">import</span> torch  <br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt     <span class="hljs-comment"># 作图  </span><br><span class="hljs-keyword">import</span> random   <span class="hljs-comment"># 产生随机数</span><br></code></pre></td></tr></table></figure><p>在这一步中，我们引入了三个库：<code>torch</code> 是 PyTorch 深度学习框架，用于构建和训练模型；<code>matplotlib.pyplot</code> 用于绘制图形，方便我们可视化数据和模型结果；<code>random</code> 用于生成随机数，在后续的数据处理中会用到。</p><h3 id="Step-2-生成数据集"><a href="#Step-2-生成数据集" class="headerlink" title="Step 2: 生成数据集"></a>Step 2: 生成数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">### 生成数据集 输入真实 w 和 b以及数据量data_num</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generateDataSet</span>(<span class="hljs-params">w, b, data_num</span>):  <br>    x = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (data_num, <span class="hljs-built_in">len</span>(w)))  <span class="hljs-comment"># std: 0, mean: 1, shape()</span><br>    y = torch.matmul(x, w) + b          <span class="hljs-comment"># matmul函数实现矩阵相乘  </span><br>  <br>    <span class="hljs-comment"># 添加噪声  </span><br>    noise = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, y.shape)  <br>    y += noise  <br>  <br>    <span class="hljs-keyword">return</span> x, y  <br><br><span class="hljs-comment"># 定义真实 w 和 b</span><br>w_true = torch.tensor([<span class="hljs-number">8.1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>])  <br>b_true = torch.tensor(<span class="hljs-number">1.1</span>)  <br>DataSetNum = <span class="hljs-number">500</span>  <br><br><span class="hljs-comment"># 生成训练集</span><br>X, Y = generateDataSet(w_true, b_true, DataSetNum)  <br><br><span class="hljs-comment"># 可视化分享</span><br>plt.scatter(X[:, <span class="hljs-number">3</span>], Y, <span class="hljs-number">1</span>)  <br>plt.show()<br></code></pre></td></tr></table></figure><p>这里我们定义了一个函数 <code>generateDataSet</code> 用于生成数据集。首先，我们使用 <code>torch.normal</code> 函数生成均值为 0，标准差为 1 的正态分布随机数作为输入特征 <code>x</code>，其形状为 <code>(data_num, len(w))</code>。然后，通过矩阵乘法 <code>torch.matmul</code> 计算 <code>y</code> 的值，即 <code>y = x * w + b</code>。接着，为了模拟真实数据中的噪声，我们给 <code>y</code> 加上一个均值为 0，标准差为 0.01 的正态分布噪声。最后，返回生成的 <code>x</code> 和 <code>y</code>。</p><h3 id="Step-3-分小批次采集数据"><a href="#Step-3-分小批次采集数据" class="headerlink" title="Step 3: 分小批次采集数据"></a>Step 3: 分小批次采集数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 取数函数, Data: 自变量， labal: 标签, batchsize: 批量大小</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">getData</span>(<span class="hljs-params">data, label, batch_size</span>):<br>    length = <span class="hljs-built_in">len</span>(label)<br>    indices = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(length))<br><br>    <span class="hljs-comment"># 打乱数据, 取数据时要打乱数据</span><br>    random.shuffle(indices)<br><br>    <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, length, batch_size):<br>        get_indices = indices[each: each + batch_size]<br>        get_data = data[get_indices]<br>        get_labal = label[get_indices]<br>        <span class="hljs-comment"># print(f&quot;batch form &#123;get_indices[0]&#125; to &#123;get_indices[-1]&#125;&quot;)</span><br>        <span class="hljs-comment"># return 直接终止函数， yield 有存档点的返回数据</span><br>        <span class="hljs-keyword">yield</span> get_data, get_labal<br><br><span class="hljs-comment"># 测试获取数据是否分批</span><br>batch_size = <span class="hljs-number">16</span><br><span class="hljs-comment"># for batch_x, batch_y in getData(X, Y, batch_size):</span><br><span class="hljs-comment">#     print(batch_x, batch_y)</span><br></code></pre></td></tr></table></figure><p><code>getData</code> 函数用于分小批次采集数据。首先，我们获取标签的长度，生成一个包含所有索引的列表 <code>indices</code>，并使用 <code>random.shuffle</code> 函数打乱这些索引。然后，通过循环每次取出 <code>batch_size</code> 个索引，根据这些索引从数据和标签中取出对应的小批次数据，使用 <code>yield</code> 关键字返回，这样可以实现数据的分批迭代</p><h3 id="Step-4-定义模型"><a href="#Step-4-定义模型" class="headerlink" title="Step 4: 定义模型"></a>Step 4: 定义模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义模型</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model</span>(<span class="hljs-params">x, w, b</span>):<br>    y_pred = torch.matmul(x, w) + b<br>    <span class="hljs-keyword">return</span> y_pred<br></code></pre></td></tr></table></figure><p>模型定义非常简单，就是一个线性模型 <code>y_pred = x * w + b</code>，其中 <code>x</code> 是输入特征，<code>w</code> 是权重，<code>b</code> 是偏置。</p><h3 id="Step-5-定义损失函数"><a href="#Step-5-定义损失函数" class="headerlink" title="Step 5: 定义损失函数"></a>Step 5: 定义损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">maeloss</span>(<span class="hljs-params">y_pred, y</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-built_in">abs</span>(y_pred - y)) / <span class="hljs-built_in">len</span>(y)<br></code></pre></td></tr></table></figure><p>这里我们使用平均绝对误差（MAE）作为损失函数，计算预测值 <code>y_pred</code> 与真实值 <code>y</code> 之间的绝对误差的平均值。</p><h3 id="Step-6-梯度下降-SGD-更新参数"><a href="#Step-6-梯度下降-SGD-更新参数" class="headerlink" title="Step 6: 梯度下降(SGD)更新参数"></a>Step 6: 梯度下降(SGD)更新参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 梯度下降</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">SGD</span>(<span class="hljs-params">params, lr</span>):<br>    <span class="hljs-comment"># 不计算梯度</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>            <span class="hljs-comment"># 无法写成 param = param - param.grad() * lr 报错</span><br>            param -= param.grad * lr<br>            <span class="hljs-comment"># 将使用过的参数梯度归零</span><br>            param.grad.zero_()<br></code></pre></td></tr></table></figure><p><code>SGD</code> 函数实现了随机梯度下降算法。在更新参数时，我们使用 <code>with torch.no_grad()</code> 上下文管理器来避免计算梯度，因为在更新参数时不需要计算梯度。对于每个参数，我们使用 <code>param -= param.grad * lr</code> 来更新参数值，其中 <code>lr</code> 是学习率。更新完参数后，使用 <code>param.grad.zero_()</code> 将参数的梯度归零，以便下一次计算梯度。</p><h3 id="Step-7-训练模型"><a href="#Step-7-训练模型" class="headerlink" title="Step 7: 训练模型"></a>Step 7: 训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">lr = <span class="hljs-number">0.03</span><br><span class="hljs-comment"># 随机生成 w, b参数的值</span><br><span class="hljs-comment"># 确定累计梯度</span><br>w = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, w_true.shape, requires_grad=<span class="hljs-literal">True</span>)<br>b = torch.tensor(<span class="hljs-number">0.01</span>, requires_grad=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># print(w, b)</span><br><br><span class="hljs-comment"># 训练轮数</span><br>epochs = <span class="hljs-number">50</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    data_loss = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> x_batch, y_batch <span class="hljs-keyword">in</span> getData(X, Y, batch_size):<br>        y_pred = model(x_batch, w, b)<br>        loss = maeloss(y_pred, y_batch)<br>        <span class="hljs-comment"># 梯度回传</span><br>        loss.backward()<br>        <span class="hljs-comment"># 更新模型</span><br>        SGD([w, b], lr)<br>        data_loss += loss<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch: %03d, loss: %.6f&quot;</span>%(epoch, data_loss))<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;真实的参数为：&quot;</span>, w_true, b_true)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练之后的参数：&quot;</span>, w, b)<br></code></pre></td></tr></table></figure><p>在训练模型部分，我们首先定义了学习率 <code>lr</code>，然后随机初始化权重 <code>w</code> 和偏置 <code>b</code>，并设置 <code>requires_grad=True</code> 以便计算梯度。接着，我们定义了训练轮数 <code>epochs</code>，在每个训练轮次中，我们遍历每个小批次的数据，计算预测值 <code>y_pred</code> 和损失 <code>loss</code>，使用 <code>loss.backward()</code> 进行梯度回传，然后调用 <code>SGD</code> 函数更新模型参数。最后，打印每个轮次的损失值以及真实参数和训练后的参数。</p><h3 id="Step-8-可视化"><a href="#Step-8-可视化" class="headerlink" title="Step 8: 可视化"></a>Step 8: 可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 可视化</span><br>idx = <span class="hljs-number">1</span><br>plt.plot(X[:, idx].detach().numpy(), X[:, idx].detach().numpy() * w[idx].detach().numpy() + b.detach().numpy())<br>plt.scatter(X[:, idx], Y, <span class="hljs-number">1</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>在可视化部分，我们选择第 <code>idx</code> 个特征，绘制出训练后的模型预测的直线和原始数据的散点图，方便我们直观地观察模型的拟合效果。</p><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><ul><li>小批次采样时要打乱数据，保证是没有顺序依赖，打乱数据<code>random.shuffle(list)</code></li><li><code>with torch.no_grad():</code>作用域下在计算图中不会累计梯度</li><li>计算完梯度使用<code>param.grad.zero_()</code>清空梯度否则后续计算梯度会累积梯度(梯度相加)</li><li>要计算梯度的参数在定义时，使用<code>requires_grad = True</code>来存储梯度</li><li>前向传播计算损失函数，后向传播更新参数</li><li>学习率<code>lr</code>较小时训练的模型loss下降缓慢，较大时loss不稳定</li><li><code>loss.backward()</code>梯度回传，此时param.grad为$\frac{\partial loss}{\partial param}$</li></ul><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">### 引入库函数  </span><br><span class="hljs-keyword">import</span> torch  <br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt     <span class="hljs-comment"># 作图  </span><br><span class="hljs-keyword">import</span> random   <span class="hljs-comment"># 产生随机数  </span><br>  <br><span class="hljs-comment">### 生成数据集 输入真实 w 和 b以及数据量  </span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generateDataSet</span>(<span class="hljs-params">w, b, data_num</span>):  <br>    x = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (data_num, <span class="hljs-built_in">len</span>(w)))  <br>    y = torch.matmul(x, w) + b          <span class="hljs-comment"># matmul函数实现矩阵相乘  </span><br>  <br>    <span class="hljs-comment"># 添加噪声  </span><br>    noise = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, y.shape)  <br>    y += noise  <br>  <br>    <span class="hljs-keyword">return</span> x, y  <br>  <br>w_true = torch.tensor([<span class="hljs-number">8.1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>])  <br>b_true = torch.tensor(<span class="hljs-number">1.1</span>)  <br>DataSetNum = <span class="hljs-number">500</span>  <br>  <br>X, Y = generateDataSet(w_true, b_true, DataSetNum)  <br>  <br>plt.scatter(X[:, <span class="hljs-number">3</span>], Y, <span class="hljs-number">1</span>)  <br>plt.show()  <br>  <br><span class="hljs-comment"># 取数函数, Data: 自变量， labal: 标签, batchsize: 批量大小  </span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">getData</span>(<span class="hljs-params">data, label, batch_size</span>):  <br>    length = <span class="hljs-built_in">len</span>(label)  <br>    indices = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(length))  <br>  <br>    <span class="hljs-comment"># 打乱数据, 取数据时要打乱数据  </span><br>    random.shuffle(indices)  <br>  <br>    <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, length, batch_size):  <br>        get_indices = indices[each: each + batch_size]  <br>        get_data = data[get_indices]  <br>        get_labal = label[get_indices]  <br>        <span class="hljs-comment"># print(f&quot;batch form &#123;get_indices[0]&#125; to &#123;get_indices[-1]&#125;&quot;)  </span><br>        <span class="hljs-comment"># return 直接终止函数， yield 有存档点的返回数据  </span><br>        <span class="hljs-keyword">yield</span> get_data, get_labal  <br>  <br><span class="hljs-comment"># 测试获取数据是否分批  </span><br>batch_size = <span class="hljs-number">16</span>  <br><span class="hljs-comment"># for batch_x, batch_y in getData(X, Y, batch_size):  </span><br><span class="hljs-comment">#     print(batch_x, batch_y)  </span><br>  <br><span class="hljs-comment"># 定义模型  </span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model</span>(<span class="hljs-params">x, w, b</span>):  <br>    y_pred = torch.matmul(x, w) + b  <br>    <span class="hljs-keyword">return</span> y_pred  <br>  <br><span class="hljs-comment"># 定义损失函数  </span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">maeloss</span>(<span class="hljs-params">y_pred, y</span>):  <br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-built_in">abs</span>(y_pred - y)) / <span class="hljs-built_in">len</span>(y)  <br>  <br><span class="hljs-comment"># 梯度下降  </span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">SGD</span>(<span class="hljs-params">params, lr</span>):  <br>    <span class="hljs-comment"># 不计算梯度  </span><br>    <span class="hljs-keyword">with</span> torch.no_grad():  <br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:  <br>            <span class="hljs-comment"># 无法写成 param = param - param.grad() * lr 报错  </span><br>            param -= param.grad * lr  <br>            <span class="hljs-comment"># 将使用过的参数梯度归零  </span><br>            param.grad.zero_()  <br>  <br>lr = <span class="hljs-number">0.03</span>  <br><span class="hljs-comment"># 随机生成 w, b参数的值  </span><br><span class="hljs-comment"># 确定累计梯度  </span><br>w = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>, w_true.shape, requires_grad=<span class="hljs-literal">True</span>)  <br>b = torch.tensor(<span class="hljs-number">0.01</span>, requires_grad=<span class="hljs-literal">True</span>)  <br><span class="hljs-comment"># print(w, b)  </span><br>  <br><span class="hljs-comment"># 训练轮数  </span><br>epochs = <span class="hljs-number">50</span>  <br>  <br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):  <br>    data_loss = <span class="hljs-number">0</span>  <br>    <span class="hljs-keyword">for</span> x_batch, y_batch <span class="hljs-keyword">in</span> getData(X, Y, batch_size):  <br>        y_pred = model(x_batch, w, b)  <br>        loss = maeloss(y_pred, y_batch)  <br>        <span class="hljs-comment"># 梯度回传  </span><br>        loss.backward()  <br>        <span class="hljs-comment"># 更新模型  </span><br>        SGD([w, b], lr)  <br>        data_loss += loss  <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;epoch: %03d, loss: %.6f&quot;</span>%(epoch, data_loss))  <br>  <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;真实的参数为：&quot;</span>, w_true, b_true)  <br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练之后的参数：&quot;</span>, w, b)  <br>  <br><span class="hljs-comment"># 可视化  </span><br>idx = <span class="hljs-number">1</span>  <br>plt.plot(X[:, idx].detach().numpy(), X[:, idx].detach().numpy() * w[idx].detach().numpy() + b.detach().numpy())  <br>plt.scatter(X[:, idx], Y, <span class="hljs-number">1</span>)  <br>plt.show()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>考研复试项目</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>线性回归</title>
    <link href="/2025/02/11/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <url>/2025/02/11/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/02/10/hello-world/"/>
    <url>/2025/02/10/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>技术</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
